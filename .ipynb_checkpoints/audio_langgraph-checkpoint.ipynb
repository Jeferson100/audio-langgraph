{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a593f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.4 environment at: C:\\Users\\jefer\\anaconda3\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m86 packages\u001b[0m \u001b[2min 550ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 118ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m25 packages\u001b[0m \u001b[2min 617ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblockbuster\u001b[0m\u001b[2m==1.5.25\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcloudpickle\u001b[0m\u001b[2m==2.2.1 (from file:///C:/Users/dev-admin/perseverance-python-buildout/croot/cloudpickle_1699480575079/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcloudpickle\u001b[0m\u001b[2m==3.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdydantic\u001b[0m\u001b[2m==0.0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mforbiddenfruit\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema-rs\u001b[0m\u001b[2m==0.29.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==0.3.27\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.27\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-groq\u001b[0m\u001b[2m==0.3.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==0.3.9\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==0.6.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==0.2.76\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-api\u001b[0m\u001b[2m==0.0.48\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.1.74\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.13\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.3.45\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.10.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyjwt\u001b[0m\u001b[2m==2.8.0 (from file:///C:/b/abs_04qhgo75wz/croot/pyjwt_1715095119685/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyjwt\u001b[0m\u001b[2m==2.10.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==2.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstructlog\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrustcall\u001b[0m\u001b[2m==0.0.39\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f109326b-af8d-43bd-8a61-2572106858bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.messages import HumanMessage\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: List[str]\n",
    "    \n",
    "def chat_groq(state):\n",
    "    message = state['messages'][-1]\n",
    "    response = chat.invoke(message)\n",
    "    return {\"messages\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9f1010f-7d57-41fd-9993-bfa11119e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.invoke(\"Tudo bem com vc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a06c91e-30b3-499b-9654-c6ca47864a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estou bem, obrigado por perguntar! Sou um modelo de linguagem treinado por máquina, então não tenho sentimentos ou emoções como os humanos, mas estou funcionando corretamente e pronto para ajudar com qualquer coisa que você precise. Como posso ajudar você hoje?'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fae5ca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': 'Estou bem, obrigado por perguntar! Sou um modelo de linguagem treinado por máquina, então não tenho sentimentos ou emoções como os humanos, mas estou funcionando corretamente e pronto para ajudar com qualquer coisa que você precise. Como posso ajudar você hoje?'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_groq({\"messages\": [\"Tudo bem com vc\"]})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebe1b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"chat_groq\", chat_groq)\n",
    "graph.add_edge(START, \"chat_groq\")\n",
    "graph.add_edge(\"chat_groq\", END)\n",
    "\n",
    "graph_compiled = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d8a7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': 'Hoje é quarta-feira, 18 de setembro de 2024.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_compiled.invoke({\"messages\": [\"que dia e hoje\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8212710",
   "metadata": {},
   "source": [
    "### Connect to your deployment\n",
    "\n",
    "Connect to your deployment using the URL endpoint:\n",
    "- **Studio**: Found in Studio UI \n",
    "- **CLI**: Printed to console (typically `http://localhost:2024`)\n",
    "- **Cloud**: Available in LangGraph Deployment page\n",
    "\n",
    "We'll connect to the deployment as a [RemoteGraph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/#how-to-interact-with-the-deployment-using-remotegraph). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b42b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.pregel.remote import RemoteGraph\n",
    "from langchain_core.messages import convert_to_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Local deployment (via LangGraph Studio)\n",
    "local_deployment_url = \"http://localhost:2024\"\n",
    "\n",
    "# Deployment URL\n",
    "cloud_deployment_url = \"https://task-maistro-1b681add7a2b549499bb0cd21a7e5be4.default.us.langgraph.app\"\n",
    "\n",
    "# Graph name\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "# Connect to the deployment\n",
    "remote_graph = RemoteGraph(graph_name, url=local_deployment_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed85d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hi I'm Lance. I live in San Francisco with my wife and have a 1 year old.\"]\n",
      "Nice to meet you, Lance! San Francisco is a great city to raise a family. What's your 1-year-old like? Are they into any fun activities or have any quirky habits?\n"
     ]
    }
   ],
   "source": [
    "# Int\n",
    "user_input = \"Hi I'm Lance. I live in San Francisco with my wife and have a 1 year old.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Test-Deployment-User\"}}\n",
    "for chunk in graph_compiled.stream({\"messages\": [user_input]}, stream_mode=\"values\", config=config):\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092a7ef",
   "metadata": {},
   "source": [
    "### Add audio\n",
    "\n",
    "Our deployed graph has some benefits: \n",
    "* It has built-in support for long-term memory \n",
    "* It implements all the logic for task mAIstro \n",
    "\n",
    "But, we have a challenge:\n",
    "* It takes test as input and returns text as output\n",
    "\n",
    "We need to add audio input and output to the graph. So, we'll simply add two nodes to our graph:\n",
    "\n",
    "1. **Audio Input Node**\n",
    "   * Records microphone input (stop with Enter)\n",
    "   * Transcribes speech using Whisper\n",
    "   * Passes text to Task mAIstro\n",
    "\n",
    "2. **Audio Output Node**\n",
    "   * Takes Task mAIstro's text response\n",
    "   * Converts to speech via ElevenLabs\n",
    "   * Plays audio response\n",
    "\n",
    "We can achieve this by embedding our deployed graph [as a node](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/#using-as-a-subgraph) in a new graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "add8f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import threading\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Image, display\n",
    "import pygame\n",
    "import tempfile\n",
    "import os\n",
    "from groq import Groq\n",
    "from langgraph.graph import StateGraph, MessagesState, END, START\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq()\n",
    "\n",
    "def record_audio_until_stop(state: State):\n",
    "    \"\"\"Records audio from the microphone until Enter is pressed, then saves it to a .wav file.\"\"\"\n",
    "    \n",
    "    audio_data = []  # List to store audio chunks\n",
    "    recording = True  # Flag to control recording\n",
    "    sample_rate = 16000  # (kHz) Adequate for human voice frequency\n",
    "    \n",
    "    def record_audio():\n",
    "        \"\"\"Continuously records audio until the recording flag is set to False.\"\"\"\n",
    "        nonlocal audio_data, recording\n",
    "        with sd.InputStream(samplerate=sample_rate, channels=1, dtype='int16') as stream:\n",
    "            print(\"Recording your instruction! ... Press Enter to stop recording.\")\n",
    "            while recording:\n",
    "                audio_chunk, _ = stream.read(1024)  # Read audio data in chunks\n",
    "                audio_data.append(audio_chunk)\n",
    "\n",
    "    def stop_recording():\n",
    "        \"\"\"Waits for user input to stop the recording.\"\"\"\n",
    "        input()  # Wait for Enter key press\n",
    "        nonlocal recording\n",
    "        recording = False\n",
    "\n",
    "    # Start recording in a separate thread\n",
    "    recording_thread = threading.Thread(target=record_audio)\n",
    "    recording_thread.start()\n",
    "\n",
    "    # Start a thread to listen for the Enter key\n",
    "    stop_thread = threading.Thread(target=stop_recording)\n",
    "    stop_thread.start()\n",
    "\n",
    "    # Wait for both threads to complete\n",
    "    stop_thread.join()\n",
    "    recording_thread.join()\n",
    "\n",
    "    # Stack all audio chunks into a single NumPy array and write to file\n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    \n",
    "    # Create a temporary WAV file for Groq API\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "        write(temp_file.name, sample_rate, audio_data)\n",
    "        temp_filename = temp_file.name\n",
    "    \n",
    "    try:\n",
    "        # Transcribe via Groq Whisper\n",
    "        with open(temp_filename, \"rb\") as file:\n",
    "            transcription = groq_client.audio.transcriptions.create(\n",
    "                file=(temp_filename, file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                prompt=\"Transcreva o áudio em português brasileiro\",\n",
    "                response_format=\"json\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        os.unlink(temp_filename)\n",
    "    \n",
    "    # Print the transcription\n",
    "    print(\"Here is the transcription:\", transcription.text)\n",
    "    \n",
    "    # Write to messages \n",
    "    return {\"messages\": [HumanMessage(content=transcription.text)]}\n",
    "\n",
    "def play_audio(state: State):\n",
    "    \"\"\"Plays the audio response using Groq TTS.\"\"\"\n",
    "    \n",
    "    # Response from the agent \n",
    "    response = state['messages'][-1]\n",
    "    \n",
    "    # Prepare text by replacing ** with empty strings\n",
    "    # These can cause unexpected behavior in TTS\n",
    "    if isinstance(response, str):\n",
    "        cleaned_text = response.replace(\"**\", \"\")\n",
    "    else:\n",
    "        cleaned_text = response.content.replace(\"**\", \"\")\n",
    "    \n",
    "    try:\n",
    "        # Call Groq text-to-speech API\n",
    "        tts_response = groq_client.audio.speech.create(\n",
    "            model=\"playai-tts\",\n",
    "            voice=\"Atlas-PlayAI\",  # You can change this to other available voices\n",
    "            input=cleaned_text,\n",
    "            response_format=\"wav\"\n",
    "        )\n",
    "        \n",
    "        # Create a temporary file to save the audio\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "            tts_response.write_to_file(temp_file.name)\n",
    "            temp_audio_file = temp_file.name\n",
    "        \n",
    "        # Play the audio using pygame\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(temp_audio_file)\n",
    "        pygame.mixer.music.play()\n",
    "        \n",
    "        # Wait for the audio to finish playing\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            pygame.time.wait(100)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in text-to-speech: {e}\")\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        if 'temp_audio_file' in locals():\n",
    "            try:\n",
    "                os.unlink(temp_audio_file)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Alternative play_audio function using sounddevice (if pygame doesn't work)\n",
    "def play_audio_alternative(state: State):\n",
    "    \"\"\"Alternative audio playback using sounddevice.\"\"\"\n",
    "    \n",
    "    response = state['messages'][-1]\n",
    "    if isinstance(response, str):\n",
    "        cleaned_text = response.replace(\"**\", \"\")\n",
    "    else:\n",
    "        cleaned_text = response.content.replace(\"**\", \"\")\n",
    "    try:\n",
    "        # Call Groq text-to-speech API\n",
    "        tts_response = groq_client.audio.speech.create(\n",
    "            model=\"playai-tts\",\n",
    "            voice=\"Arista-PlayAI\",\n",
    "            input=cleaned_text,\n",
    "            response_format=\"wav\"\n",
    "        )\n",
    "        \n",
    "        # Create temporary file and read audio data\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "            tts_response.write_to_file(temp_file.name)\n",
    "            temp_audio_file = temp_file.name\n",
    "        \n",
    "        # Read the WAV file and play with sounddevice\n",
    "        from scipy.io.wavfile import read\n",
    "        sample_rate, audio_data = read(temp_audio_file)\n",
    "        sd.play(audio_data, sample_rate)\n",
    "        sd.wait()  # Wait until the audio finishes playing\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in text-to-speech: {e}\")\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'temp_audio_file' in locals():\n",
    "            try:\n",
    "                os.unlink(temp_audio_file)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.messages import HumanMessage\n",
    "load_dotenv()\n",
    "chat = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: List[str]\n",
    "    \n",
    "def chat_groq(state):\n",
    "    message = state['messages'][-1]\n",
    "    #human_message = HumanMessage(content=message)\n",
    "    response = chat.invoke([message])\n",
    "    return {\"messages\": response.content}\n",
    "\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"audio_input\", record_audio_until_stop)\n",
    "graph.add_node(\"chat_groq\", chat_groq)\n",
    "graph.add_node(\"audio_output\", play_audio)\n",
    "\n",
    "graph.add_edge(START, \"audio_input\")\n",
    "graph.add_edge(\"audio_input\", \"chat_groq\")\n",
    "graph.add_edge(\"chat_groq\", \"audio_output\")\n",
    "graph.add_edge(\"audio_output\", END)\n",
    "\n",
    "graph_compiled = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ddedf-8d6a-43a4-af89-43ef6adda62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph_compiled.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ad3a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "29eb1542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Follow the user's instructions:\" additional_kwargs={} response_metadata={}\n",
      "Recording your instruction! ... Press Enter to stop recording.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the transcription:  Olá, tudo bem?\n",
      "[HumanMessage(content=' Olá, tudo bem?', additional_kwargs={}, response_metadata={})]\n",
      "Olá! Sim, estou funcionando perfeitamente, obrigado por perguntar. Estou aqui para ajudar com qualquer coisa que você precise. Como posso te ajudar hoje?\n",
      "Error in text-to-speech: Error code: 500 - {'error': {'message': 'Internal Server Error', 'type': 'internal_server_error'}}\n"
     ]
    }
   ],
   "source": [
    "# Set user ID for storing memories\n",
    "config = {\"configurable\": {\"user_id\": \"Test-Audio-UX\", \"thread_id\": thread_id}}\n",
    "\n",
    "initial_state = {\"messages\": [\"Olá, como você está?\"]}\n",
    "for chunk in graph_compiled.stream({\"messages\":HumanMessage(content=\"Follow the user's instructions:\")}, stream_mode=\"values\", config=config):\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b3c0304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Follow the user's instructions:\" additional_kwargs={} response_metadata={}\n",
      "Recording your instruction! ... Press Enter to stop recording.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the transcription:  sobre qual time eu perguntei anteriormente\n",
      "[HumanMessage(content=' sobre qual time eu perguntei anteriormente', additional_kwargs={}, response_metadata={})]\n",
      "Esta é a primeira mensagem da nossa conversa, então não tenho informações sobre uma pergunta anterior sobre um time. Como posso ajudá-lo hoje?\n",
      "Error in text-to-speech: Error code: 500 - {'error': {'message': 'Internal Server Error', 'type': 'internal_server_error'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph_compiled.stream({\"messages\":HumanMessage(content=\"Follow the user's instructions:\")}, stream_mode=\"values\", config=config):\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81914f-0c5e-4708-9511-caaefdae0468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
